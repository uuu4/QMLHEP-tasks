{"cells":[{"cell_type":"code","metadata":{"source_hash":"c1f0503f","execution_start":1741128577443,"execution_millis":0,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"e8be756d219e4af695e91147dd7163c4","deepnote_cell_type":"code"},"source":"# task 4 \n#!pip install cirq tensorflow_quantum tensorflow\n#!pip install sympy\nimport cirq,sympy\nimport tensorflow_quantum as tfq\nimport tensorflow as tf\nimport numpy as np","block_group":"e8be756d219e4af695e91147dd7163c4","execution_count":43,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b32cad3c","execution_start":1741126575819,"execution_millis":1,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"b3f3bca67b3049d8aab2faddf49a514c","deepnote_cell_type":"code"},"source":"import numpy as np\n\n# load the NPZ file\ndata = np.load('QIS_EXAM_200Events.npz',allow_pickle=True)\n\n# see the keys\nprint(data.keys())","block_group":"af822235e9a24c17af7557303d3b78d2","execution_count":10,"outputs":[{"name":"stdout","text":"KeysView(NpzFile 'QIS_EXAM_200Events.npz' with keys: training_input, test_input)\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/62536682-7d73-4ff3-95b6-d346369d81e8","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2b55bce3","execution_start":1741126578314,"execution_millis":151,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"f4278ff9a58a49448ac5f0f155053794","deepnote_cell_type":"code"},"source":"# we cannot clearly see the signal and background, so it should be emmbedded inside it\n\ntraining_obj = data['training_input'].item()\ntest_obj = data['test_input'].item()  # extract the underlying object\nprint(type(training_obj))\nprint(type(test_obj))\nprint(training_obj.keys() if hasattr(training_obj, 'keys') else training_obj)\n\n# now we can combine the data since its its two different objects\nX_train = np.concatenate((training_obj['0'], training_obj['1']))\ny_train = np.concatenate((np.zeros(len(training_obj['0'])), np.ones(len(training_obj['1']))))\n\n# combine test data\nX_test = np.concatenate((test_obj['0'], test_obj['1']))\ny_test = np.concatenate((np.zeros(len(test_obj['0'])), np.ones(len(test_obj['1']))))","block_group":"233bf7a35ea64084957e15aac4add640","execution_count":12,"outputs":[{"name":"stdout","text":"<class 'dict'>\n<class 'dict'>\ndict_keys(['0', '1'])\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/7131a414-1994-4f96-b223-b414eaee0781","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2b5f5381","execution_start":1741127543062,"execution_millis":3,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"dcbec7333ad6460dab3e63bc153a1ea5","deepnote_cell_type":"code"},"source":"# now its time encode the data :)))\n# since our data is already normalized between -1 and 1, we can map a value x in [-1,1] to an angle θ in [0,π]\n\ndef create_angle_embedding_circuit(qubits, features):\n    circuit = cirq.Circuit()\n    for i, qubit in enumerate(qubits):\n        # scaling data value to angle\n        angle = features[i] * np.pi\n        # apply an RX gate with the calculated angle\n        circuit.append(cirq.rx(angle)(qubit))\n    return circuit\n\n# one qubit per feature to encode the data\nnum_qubits = 5\nqubits = [cirq.GridQubit(0, i) for i in range(num_qubits)]\n\n\nsample_features = X_train[0]\n\n# creating the encoding circuit\ncircuit = create_angle_embedding_circuit(qubits, sample_features)\nprint(circuit)","block_group":"ef615e06f2c44b3d879aaa80e22cf067","execution_count":37,"outputs":[{"name":"stdout","text":"(0, 0): ───Rx(-0.431π)───\n\n(0, 1): ───Rx(0.868π)────\n\n(0, 2): ───Rx(-0.926π)───\n\n(0, 3): ───Rx(-0.927π)───\n\n(0, 4): ───Rx(-0.569π)───\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/94f92c33-4f3c-440a-9b7d-6ee09a9b52bb","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"88591905","execution_start":1741128581709,"execution_millis":63,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"be036b431d0b4b3eb01bb9e26465963a","deepnote_cell_type":"code"},"source":"# the classical data now encoded into quantum states, now we can do the fun part (QGAN)!!!!!!!\n# first, lets start with discriminators\n\n\n# for further rotations, we need to create a parametrized circuit\ndef create_parametrized_circuit(qubits, symbols):\n    circuit = cirq.Circuit()\n    for i, qubit in enumerate(qubits):\n        circuit.append(cirq.ry(symbols[i])(qubit))\n    return circuit\n\n# Example: define symbols for 5 qubits (adjust number as needed)\nsymbols = sympy.symbols('theta0:5')\n\n# combining the encoding and parameterized circuit like its magic!!!!\nencoding_circuit = create_angle_embedding_circuit(qubits, sample_features)\nparam_circuit = create_parametrized_circuit(qubits, symbols)\nfull_discriminator_circuit = encoding_circuit + param_circuit\n\n# define an observable to measure; for instance, measure Z(it is not important though) on the first qubit ; \nobservable = cirq.Z(qubits[0])\n\n# creating TFQ layer for the discriminator (its for calculating expected value and optimizing trainable params)\ndiscriminator_model = tf.keras.Sequential([\n    tfq.layers.PQC(full_discriminator_circuit, observable),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # to output a probability\n])\n\ndiscriminator_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                            loss='binary_crossentropy',\n                            metrics=['accuracy'])","block_group":"775bcb95e13242b18923480a3ef98267","execution_count":45,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e814f461","execution_start":1741132470449,"execution_millis":3,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"8fc9186875b84333bde86581a4c2f913","deepnote_cell_type":"code"},"source":"# after wrapping the model with TFQ, its time for generator!\n# we must ensure that the generator produces a noise value, then we will be cool to wrap it with TFQ too ;)\n\nqubits = [cirq.GridQubit(0, i) for i in range(5)]\n\ndef create_generator_circuit(qubits, symbols):\n    circuit = cirq.Circuit()\n    \n    for i, qubit in enumerate(qubits):\n        circuit.append(cirq.ry(symbols[i])(qubit))\n    # P.S : I did not see any reason to entangle the qubits.\n    return circuit\n\n# one qubit per feature as usual\ngen_symbols = sympy.symbols('phi0:5')\n\ngenerator_circuit = create_generator_circuit(qubits, gen_symbols)\nprint(\"Generator circuit:\")\nprint(generator_circuit)\n","block_group":"a05bb1ba1811484a84d0144a7202f4ef","execution_count":86,"outputs":[{"name":"stdout","text":"Generator circuit:\n(0, 0): ───Ry(phi0)───\n\n(0, 1): ───Ry(phi1)───\n\n(0, 2): ───Ry(phi2)───\n\n(0, 3): ───Ry(phi3)───\n\n(0, 4): ───Ry(phi4)───\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/bac9bb36-a2ce-48f4-9528-4d6c70ab8ce1","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"5f7e6ccf","execution_start":1741132474591,"execution_millis":0,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"032624a7a71b494a8530f200d442ad99","deepnote_cell_type":"code"},"source":"# wrapping generator with TFQ\ngenerator_model = tf.keras.Sequential([\n    tfq.layers.PQC(generator_circuit, cirq.Z(qubits[0])),\n    tf.keras.layers.Dense(5, activation='tanh') \n])\n\ngenerator_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n                        loss='binary_crossentropy') ","block_group":"8bec93c322434daa807b5fb076704c67","execution_count":88,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"f94a00dc","execution_start":1741132476773,"execution_millis":4,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"f902b241d55940ff88afa770772824f7","deepnote_cell_type":"code"},"source":"# before the training cycle, we need to define some helper functions:\n\ndef get_real_data_batch(batch_size):\n    indices = np.random.choice(len(X_train), batch_size, replace=False)\n    return X_train[indices]\n\ndef generate_noise(batch_size, noise_dim):\n    # noise_dim = size of the noise vector\n    return np.random.normal(0, 1, (batch_size, noise_dim))","block_group":"092e0cbf3186437ca9bd35cab181dc64","execution_count":90,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"906b7556","execution_start":1741132872653,"execution_millis":201,"execution_context_id":"ecb4626d-9aff-4dba-97ad-94433f73c51c","cell_id":"9527ba3b30a144e4831b0b411dad0944","deepnote_cell_type":"code"},"source":"# ping-pong begins here\n# some parameters\nnum_epochs = 1000\nnoise_dim = 5 # In classical GANs, usually noise_dim is around 100 but in this case i prefer to go with feature size\nbatch_size = 16 # I wouldn't use this if i had a Google Colab Pro\nsteps_per_epoch = int(len(X_train) / batch_size)\n\n# training cycle\nfor epoch in range(num_epochs):\n    d_loss_epoch = [] # discriminator loss\n    g_loss_epoch = [] # generator loss\n    \n    for step in range(steps_per_epoch):\n        # ---------------------\n        #1. First we need to train the discriminator\n        # ---------------------\n        real_data = get_real_data_batch(batch_size)\n        # creating fake date with generator using random noise\n        noise = generate_noise(batch_size, noise_dim)  # random noise\n        fake_data = generator_model.predict(noise)\n        \n        # combine real and fake data to label them, 1 for real, 0 for fake\n        combined_data = np.concatenate([real_data, fake_data], axis=0)\n        combined_labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))], axis=0)\n        \n        # train the discriminator\n        d_loss = discriminator_model.train_on_batch(combined_data, combined_labels)\n        d_loss_epoch.append(d_loss)\n        \n        # ---------------------------\n        # 2. Now, generator training part\n        # ---------------------------\n        # since generator's goal is to fool the discriminator, it uses misleading labels (1)\n\n        noise = generate_noise(batch_size,noise_dim)\n        misleading_labels = np.ones((batch_size, 1))\n        \n        # Thanks to TFQ, discriminators weights are not updated during generator training but only generator weights are updated\n        g_loss = combined_model.train_on_batch(noise, misleading_labels)\n        g_loss_epoch.append(g_loss)\n    \n    # saving a report every epoch\n    avg_d_loss = np.mean(d_loss_epoch)\n    avg_g_loss = np.mean(g_loss_epoch)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Discriminator Loss: {avg_d_loss:.4f}, Generator Loss: {avg_g_loss:.4f}\")","block_group":"be7783c4992a4271bb75614943003d95","execution_count":102,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"in user code:\n\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filemy2hksps.py\", line 11, in tf__call\n        circuit_batch_dim = ag__.converted_call(ag__.ld(tf).gather, (ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(inputs),), None, fscope), 0), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'pqc_9' (type PQC).\n    \n    in user code:\n    \n        File \"/root/venv/lib/python3.10/site-packages/tensorflow_quantum/python/layers/high_level/pqc.py\", line 297, in call  *\n            circuit_batch_dim = tf.gather(tf.shape(inputs), 0)\n    \n        TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float32'> to <dtype: 'string'> (Tensor is: <tf.Tensor 'IteratorGetNext:1' shape=(None, 5) dtype=float32>)\n    \n    \n    Call arguments received by layer 'pqc_9' (type PQC):\n      • inputs=('tf.Tensor(shape=(None,), dtype=string)', 'tf.Tensor(shape=(None, 5), dtype=float32)')\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[102], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# creating fake data with generator using random noise\u001b[39;00m\n\u001b[1;32m     25\u001b[0m noise \u001b[38;5;241m=\u001b[39m generate_noise(batch_size, noise_dim)  \u001b[38;5;66;03m# random noise\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m fake_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdummy_circuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# combine real and fake data to label them, 1 for real, 0 for fake\u001b[39;00m\n\u001b[1;32m     29\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([real_data, fake_data], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m~/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/tmp/__autograph_generated_file934jx6rh.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/tmp/__autograph_generated_filemy2hksps.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 11\u001b[0m circuit_batch_dim \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgather, (\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m tiled_up_model \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtile, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_model_circuit, [ag__\u001b[38;5;241m.\u001b[39mld(circuit_batch_dim)]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m model_appended \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_append_layer, (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(append\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(tiled_up_model)), fscope)\n","\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/root/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filemy2hksps.py\", line 11, in tf__call\n        circuit_batch_dim = ag__.converted_call(ag__.ld(tf).gather, (ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(inputs),), None, fscope), 0), None, fscope)\n\n    TypeError: Exception encountered when calling layer 'pqc_9' (type PQC).\n    \n    in user code:\n    \n        File \"/root/venv/lib/python3.10/site-packages/tensorflow_quantum/python/layers/high_level/pqc.py\", line 297, in call  *\n            circuit_batch_dim = tf.gather(tf.shape(inputs), 0)\n    \n        TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float32'> to <dtype: 'string'> (Tensor is: <tf.Tensor 'IteratorGetNext:1' shape=(None, 5) dtype=float32>)\n    \n    \n    Call arguments received by layer 'pqc_9' (type PQC):\n      • inputs=('tf.Tensor(shape=(None,), dtype=string)', 'tf.Tensor(shape=(None, 5), dtype=float32)')\n"]}],"outputs_reference":"s3:deepnote-cell-outputs-production/dc54edf5-bc7e-45f7-9553-897764173529","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"3f4950c4343546d89966b6901f281686","deepnote_cell_type":"code"},"source":"","block_group":"83c09b2a520141e7b24ce111e4111041","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=15ae0520-e0c3-4a6b-83df-cfc388c1a187' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"f5a042b8d6404cc28433f28849433398"}}
